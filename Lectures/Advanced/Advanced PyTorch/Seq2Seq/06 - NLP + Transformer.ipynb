{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Deep Learning - NLP + TorchText + Embedding + Transformer\n",
    "\n",
    "Here we shall look at the Transformer architecture which is considered one of the most influential paper in recent years (https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "In addition, of course, we can use the Transformer for our sentiment analysis (i.e., use only the Encoder part of Transformer).  But to showcase the whole architecture, we shall explore the language translation problem which requires a more complicated encoder-decoder architecture.\n",
    "\n",
    "<img src = \"../figures/transformer.png\" width=\"700\">\n",
    "\n",
    "First off, to understand Transformer, here are the list of knowledge you need to know:\n",
    "\n",
    "1. Encoder-Decoder\n",
    "2. Embedding\n",
    "3. Positional Encodings\n",
    "4. Creating Masks\n",
    "5. Multi-Head Attention\n",
    "6. Feed-Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoder-Decoder architecture\n",
    "\n",
    "Encoder-decoder architecture is a very simple idea in which input is first feed into the encoder which results in a latent vector (e.g., in the case of RNN, it is the final hidden state).  Then this latent vector is feeded into the decoder state to decode the required results.\n",
    "\n",
    "Relating to our previous sentiment analysis problem (many to one), our decoder can be seen as a simple linear layer outputting class probabilities.  As for language translation, since the output is sequential, we require a decoder that output sequential data (many to many).\n",
    "\n",
    "In RNN, an encoder-decoder architecture for language translation looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module): #a.k.a seq2seq\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        #input_dim IS the same as vocab size, so please don't get confused here\n",
    "        self.encoder = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        #if your output is also text, then output_dim IS the same as input_dim \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x : [batch-size, seq len, embed size or vocab size]\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_dim).to(device)\n",
    "        _, encoder_hn = self.encoder(x, h0)\n",
    "        # encoder_hn : [num_layers(=1) * num_directions(=1), batch_size, hidden_dim]\n",
    "\n",
    "        outputs, _ = self.decoder(y, encoder_hn)\n",
    "        # outputs : [batch_size, seq len, num_directions(=1) * hidden_dim]\n",
    "\n",
    "        model = self.fc(outputs) # model : [batch_size, seq len, output_dim]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that if we are not doing language translation, our decoder is simply the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): #a.k.a normal RNN\n",
    "    def __init__(self):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_dim, num_classes)  #<---for those who like to generalize, you can also imagine this as the decoder\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x : [batch-size, seq len, embed size or vocab size]\n",
    "        h0 = torch.zeros(num_layers, batch_size, hidden_dim).to(device)\n",
    "        _, encoder_hn = self.encoder(x, h0)\n",
    "        # encoder_hn : [num_layers(=1) * num_directions(=1), batch_size, hidden_dim]\n",
    "\n",
    "        encoder_hn = encoder_hn.squeeze(0)\n",
    "        # encoder_hn : [batch_size, hidden_dim]\n",
    "\n",
    "        model = self.decoder(encoder_hn) # model : [batch_size, num_classes]\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transformer, it also has the similar encoder decoder but more sophisticated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding\n",
    "\n",
    "You probably already know this from our previosu classes, but for the sake of no prerequired knowledge for reading this lecture, embedding is simply the idea of having a dedicated vector for each tokenized unit.  These vectors, when trained properly, contains useful semantic meanings.   To train such embedding, we can simply use <code>nn.Embedding</code>, where these embedding vectors will be learnt as a parameter using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Positional Encodings\n",
    "\n",
    "Since transformer is not using recurrence, thus it does not contain order information.  Hence, it will be beneficial if we add **positional encodings** to the input embeddings.  Obviously, the positional encodings should have the same dimension <code>embed_size</code> as the embeddings, so that the two can be summed.  In the original paper, the embed size is 512.\n",
    "\n",
    "<img src = \"../figures/posenc.png\" width=\"300\">\n",
    "\n",
    "#### Many choices of positional encodings\n",
    "\n",
    "- **Use [0, 1]**: We can encode each position in the range of 0 and 1.  For example, given three words, pos1 has value of 0, pos2 has value of 0.5, and pos3 has value of 1.  Anyhow, this method yield inconsistent meaning for different length sentences.  That is, the deltas between two length is not consistent, which can confuse the model.\n",
    "\n",
    "- **Use numberings**:  We can encode each position simply by numbering 1, 2, 3 and so on.  The problem with this approach is that the model cannot generalize in case that some testing sentences are longer than training sentences.\n",
    "\n",
    "- **Use sine/cosine wave**:  In transformer, the authors have use sine and cosine functions of different frequencies, where <code>pos</code> refers to the position, $i$ refers to the value of the vector indexed at $i$.  Note that the value ranged from [-1, 1].                                                                   \n",
    "\n",
    "$$ \\text{PE}_{(\\text{pos},2i)} = \\sin(\\frac{\\text{pos}} {10000^{\\frac{2i}{\\text{embed_size}}}})$$                               \n",
    "$$ \\text{PE}_{(\\text{pos},2i+1)} = \\cos(\\frac{\\text{pos}} {10000^{\\frac{2i}{\\text{embed_size}}}}) $$                                               \n",
    "This is how the positional encoding looks like:\n",
    "\n",
    "<img src = \"../figures/pos2.png\" width=\"300\">\n",
    "\n",
    "#### Intuition behind?\n",
    "\n",
    "You may wonder how this combination of sines and cosines could represent a position?  It is actually quite simple.  Suppose you write number in binary format, it looks like this:\n",
    "\n",
    "<img src = \"../figures/pos3.png\" width=\"200\">\n",
    "\n",
    "You can spot the change in different bits.  But using binary alues would be a waste of space in the world of floats.  So instead, we can use sine/cosine functions to alternate bits.  By decreasing their frequencies, we can go from red bits to orange bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, embed_size):\n",
    "    \n",
    "    def pe(pos, i):\n",
    "        return pos / np.power(10000, 2 * (i // 2) / embed_size)\n",
    "    \n",
    "    def get_pe_vec(pos):\n",
    "        return [pe(pos, i) for i in range(embed_size)]\n",
    "\n",
    "    pe_vec = np.array([get_pe_vec(pos) for pos in range(seq_len)])\n",
    "    #pe_vec: [seq_len, embed_size]\n",
    "        \n",
    "    pe_vec[:, 0::2] = np.sin(pe_vec[:, 0::2])  # dim 2i taking step=2 from 0\n",
    "    pe_vec[:, 1::2] = np.cos(pe_vec[:, 1::2])  # dim 2i+1 taking step=2 from 1\n",
    "    return torch.FloatTensor(pe_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  1.5783e-01,  9.8747e-01,  2.5116e-02,\n",
       "          9.9968e-01,  3.9811e-03,  9.9999e-01,  6.3096e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  3.1170e-01,  9.5018e-01,  5.0217e-02,\n",
       "          9.9874e-01,  7.9621e-03,  9.9997e-01,  1.2619e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  4.5775e-01,  8.8908e-01,  7.5285e-02,\n",
       "          9.9716e-01,  1.1943e-02,  9.9993e-01,  1.8929e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  5.9234e-01,  8.0569e-01,  1.0031e-01,\n",
       "          9.9496e-01,  1.5924e-02,  9.9987e-01,  2.5238e-03,  1.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe_vec = get_positional_encoding(5, 10)\n",
    "pe_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the similarity of the 1st and 2nd word is high, so cosine similarity is high while the distance is far between 1st and 5th word, hence the cosine similarity is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simiarlity between pos 0 and 1:  0.9054891467094421\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "similarity_between_0_1 = 1 - spatial.distance.cosine(pe_vec[0], pe_vec[1])\n",
    "print(\"Simiarlity between pos 0 and 1: \", similarity_between_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simiarlity between pos 0 and 5:  0.629374623298645\n"
     ]
    }
   ],
   "source": [
    "similarity_between_0_5 = 1 - spatial.distance.cosine(pe_vec[0], pe_vec[4])\n",
    "print(\"Simiarlity between pos 0 and 5: \", similarity_between_0_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating masks\n",
    "\n",
    "Masking plays an important role in the transformer. It serves two purposes:\n",
    "- In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences.\n",
    "- In the decoder: To prevent the decoder ‘peaking’ ahead at the rest of the translated sentence when predicting the next word.\n",
    "\n",
    "Creating the mask for the input is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  6,  1,  0,  0],\n",
       "        [23,  5,  0,  0,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "x = torch.tensor([[1, 6, 1, 0, 0], [23, 5, 0, 0, 0]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True],\n",
       "        [False, False,  True,  True,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_mask = x.data.eq(0)\n",
    "pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[          1,           6,           1, -1000000000, -1000000000],\n",
       "        [         23,           5, -1000000000, -1000000000, -1000000000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_masked = x.masked_fill(pad_mask, -1e9)\n",
    "x_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to input the same encoder inputs many times to predict each of the output word, we can repeat this padding by doing like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False,  True,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_repeating_mask = x.data.eq(0).unsqueeze(1)\n",
    "pad_repeating_mask = pad_repeating_mask.expand(batch_size, seq_len, seq_len)\n",
    "pad_repeating_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, for the decoder input, which is the target text, we shal apply the same padding mask.  But in addition, recall that the decoder predicts each output word by making use of all encoder outputs and the target sentence only up until the point of each word its predicting.   \n",
    "\n",
    "Therefore we need to prevent the first output predictions from being able to see later into the sentence.  For this, we apply another additional mask to the target sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "shape = [batch_size, seq_len, seq_len]\n",
    "block_future_mask = np.triu(np.ones(shape), k=1)\n",
    "block_future_mask = torch.from_numpy(block_future_mask).byte()   #we didn't reverse here; instead we shift the operation to Encoder\n",
    "block_future_mask = block_future_mask.data.eq(0) #reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_future_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multihead Attention\n",
    "\n",
    "Recall the multihead attention in the figure:\n",
    "\n",
    "<img src = \"../figures/transformer.png\" width=\"700\">\n",
    "\n",
    "V, K and Q stand for 'key', 'value' and 'query'. These are terms used in attention functions, but honestly, V, K and Q are simply be identical copies of the embedding vector (plus positional encoding). They will have the dimensions <code>batch_size * seq_len * embed_size.</code>\n",
    "\n",
    "In multi-head attention we split the embedding vector into N heads, so they will then have the dimensions <code>batch_size * n_heads * seq_len * (embed_size / N) </code>.  This final dimension <code>( embed_size / N )</code> we will refer to as <code>d_k</code>.\n",
    "\n",
    "\n",
    "Let’s see the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(embed_size, embed_size)\n",
    "        self.W_K = nn.Linear(embed_size, embed_size)\n",
    "        self.W_V = nn.Linear(embed_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, embed_size)\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # Q: [batch_size x seq_len x embed_size], K: [batch_size x seq_len x embed_size], V: [batch_size x seq_len x embed_size]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads (i.e., from embed_size to n_heads * d_k)\n",
    "        q = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q: [batch_size x n_heads x seq_len x d_k]\n",
    "        k = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k: [batch_size x n_heads x seq_len x d_k]\n",
    "        v = self.W_V(V).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # v: [batch_size x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # repeat the attention mask n_heads time\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x seq_len x seq_len]\n",
    "\n",
    "        # calculate attention\n",
    "        # context: [batch_size x n_heads x seq_len x d_k], attn: [batch_size x n_heads x seq_len x seq_len]\n",
    "        context, attn = ScaledDotProductAttention()(q, k, v, attn_mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_k) # context: [batch_size x seq_len x n_heads * d_k]\n",
    "        output = self.linear(context)\n",
    "        \n",
    "        return self.layer_norm(output + residual), attn # output: [batch_size x seq_len x embed_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#Contiguous is related to whether the data is stored in a consecutive (a.k.a contiguous blocks) blocks.\n",
    "#Most data is contiguous, however, for example, after you transpose, the python does not really \n",
    "#rearrange the memory, but just modify the metadata, so the data loses contiguity.\n",
    "\n",
    "#Some functions such as 'view()' requires the tensor to be contiguous, thus before calling view() after\n",
    "#tranposing, we can call 'contiguous' to tell python to rearrange everything\n",
    "\n",
    "def check(tensor):\n",
    "    print(tensor.is_contiguous())\n",
    "    \n",
    "t = torch.randn(10, 10)    \n",
    "check(t) #true\n",
    "t = t.transpose(0, 1) \n",
    "check(t) #false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using view:  tensor([1., 1., 1., 1., 1., 1.])\n",
      "Using reshape:  tensor([1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#Reshap()e vs. view()\n",
    "#Two main differences:  view() expects a contiguous tensor while reshape does not\n",
    "#                       view() shares same memory with the original tensor, while reshape does not\n",
    "\n",
    "#Thus:\n",
    "#If you just want to reshape, use torch.reshape. \n",
    "#If you're also concerned about memory usage and want to ensure that the two tensors share the same data, use torch.view.\n",
    "\n",
    "z = torch.zeros(3, 2)\n",
    "view = z.view(6)\n",
    "reshape = z.reshape(6)\n",
    "z.fill_(1)\n",
    "\n",
    "print(\"Using view: \", view)  #guarantee a copy\n",
    "print(\"Using reshape: \", reshape)  #does not guarantee a copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scaled dot product attention, it is simply this equation\n",
    "\n",
    "$$                                                                         \n",
    "   \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V               \n",
    "$$                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x seq_len x seq_len]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        #attn: [batch_size x n_heads x seq_len x seq_len]\n",
    "        #V:  [batch_size x n_heads x seq_len x d_k]\n",
    "        #context = attn @ V = [batch_size x n_heads x seq_len x d_k]\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward network\n",
    "\n",
    "This part just consists of two operations, with a relu and dropout operation in between them.  Note that we can use <code>nn.Linear</code> in place of <code>nn.Conv1d</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=embed_size, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=embed_size, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        residual = inputs # inputs : [batch_size, seq_len, embed_size]\n",
    "        output = nn.ReLU()(self.conv1(inputs.transpose(1, 2)))\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "If you understand every component on the top, the rest is easy.\n",
    "\n",
    "We will now just bundle everything together in <code>EncoderLayer</code> and <code>DecoderLayer</code>.  Lastly, we can stack many of them inside a class <code>Encoder</code> and <code>Decoder</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this attention mask will be apply after Q @ K^T thus the shape will be batch, seq_len, seq_len\n",
    "def get_pad_mask(input1, input2):  #<---basically Q, K\n",
    "    batch_size, seq_len = input1.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_mask = input1.data.eq(0).unsqueeze(1)  # batch_size x 1 x seq_len; we unsqueeze so we can make expansion below\n",
    "    return pad_mask.expand(batch_size, seq_len, seq_len)  # batch_size x seq_len x seq_len\n",
    "\n",
    "def get_block_future_mask(input1):\n",
    "    mask_shape = [input1.size(0), input1.size(1), input1.size(1)] #batch_size x seq_len x seq_len\n",
    "    block_future_mask = np.triu(np.ones(mask_shape), k=1)\n",
    "    block_future_mask = torch.from_numpy(block_future_mask).byte()  \n",
    "    return block_future_mask\n",
    "    #we didn't reverse here because this mask will be added first with the padding mask and then\n",
    "    #input into get_pad_mask again.  The eq(0) will reverse for us.  Search for ****\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.ffn = FeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, pad_mask):\n",
    "        q, k, v = enc_inputs, enc_inputs, enc_inputs\n",
    "        enc_outputs, attn = self.enc_self_attn(q, k, v, pad_mask)\n",
    "        enc_outputs = self.ffn(enc_outputs) # enc_outputs: [batch_size x seq_len x embed_size]\n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_general_attn = MultiHeadAttention()  #finding attention between input and output\n",
    "        self.ffn = FeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, pad_mask, block_future_pad_mask):\n",
    "        q, k, v = dec_inputs, dec_inputs, dec_inputs\n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(q, k, v, pad_mask)\n",
    "        dec_outputs, dec_general_attn = self.dec_general_attn(dec_outputs, enc_outputs, enc_outputs, block_future_pad_mask)\n",
    "        dec_outputs = self.ffn(dec_outputs)\n",
    "        return dec_outputs, dec_self_attn, dec_general_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_positional_encoding(src_len+1, embed_size),freeze=True)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs : [batch_size x source_len]\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))\n",
    "        enc_self_attn_mask = get_pad_mask(enc_inputs, enc_inputs)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_positional_encoding(tgt_len+1, embed_size),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): # dec_inputs : [batch_size x target_len]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))\n",
    "        dec_self_attn_pad_mask = get_pad_mask(dec_inputs, dec_inputs)\n",
    "        dec_self_attn_block_future_mask = get_block_future_mask(dec_inputs)\n",
    "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_block_future_mask), 0)\n",
    "        dec_enc_attn_mask = get_pad_mask(dec_inputs, enc_inputs) #<--****\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(embed_size, tgt_vocab_size, bias=False)\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        dec_logits = self.projection(dec_outputs) # dec_logits : [batch_size x src_vocab_size x tgt_vocab_size]\n",
    "        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Try to load the Multi30K dataset and apply the model on it.  Make any necessary changes to the model if needed.\n",
    "\n",
    "https://pytorch.org/text/stable/datasets.html#multi30k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "    \n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "- http://peterbloem.nl/blog/transformers\n",
    "- https://blog.floydhub.com/the-transformer-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
