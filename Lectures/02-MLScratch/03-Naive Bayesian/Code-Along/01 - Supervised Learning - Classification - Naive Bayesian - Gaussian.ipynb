{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Supervised Learning - Classification - Naive Bayesian - Gaussian\n",
    "\n",
    "### Readings: \n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Classification\n",
    "\n",
    "In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(y | x)$ (also known as **posteriors**).\n",
    "Bayes's theorem tells us how to express this in terms of quantities as:\n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(x|y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "The proof is as follows:\n",
    "\n",
    "- the probabilty of two events x and y happening, $P(x \\cap y)$ is the probability of $x$ or $P(x)$, times the probability of $y$ given that $P(x)$ has occured, $P(y \\mid x)$\n",
    "\n",
    "$$ P(x \\cap y) = P(x)P(y \\mid x)$$\n",
    "\n",
    "- on the other hand, the probability of $x$ and $y$ is also equal to the probability of $y$ timese the probabilty of $x$ given $y$\n",
    "\n",
    "$$ P(x \\cap y) = P(y)P(x \\mid y)$$\n",
    "\n",
    "- Equating the two yields:\n",
    "\n",
    "$$ P(x)P(y \\mid x) = P(y)P(x \\mid y)$$\n",
    "\n",
    "- Thus\n",
    "\n",
    "$$ P(y \\mid x) = \\frac{P(y)P(x \\mid y)}{P(x)}$$\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Thus, if we know all these three terms on the right, we can find $P(y \\mid x)$ (posteriors).  Since if we want to use for classification, we can simply compare the upper term, thus we need to know two terms!  The $P(y)$ (priors) and $P(x \\mid y)$ (likelihoods or conditional probability).\n",
    "\n",
    "$P(y)$ (also known as **priors**) is simply\n",
    "\n",
    "$$P(y = 1) = \\frac{\\sum_{i=1}^m 1(y=1)}{m}$$\n",
    "\n",
    "$$P(y = 0) = \\frac{\\sum_{i=1}^m 1(y=0)}{m}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(x \\mid y)$ (also known as **likelihoods** or **conditional probability**) is a little bit tricky but if we are willing to make a \"naive\" assumption, then we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.  Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes.  In this classifier, the assumption is that *data from each label is drawn from a simple Gaussian distribution* as follows:\n",
    "\n",
    "$$ P(x \\mid y=1 ; \\mu_1, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_1)^{2}}{2\\sigma^{2}}}$$\n",
    "$$ P(x \\mid y=0 ; \\mu_0, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_0)^{2}}{2\\sigma^{2}}}$$\n",
    "\n",
    "where\n",
    "\n",
    "The mean of feature $j$ when $y=0$ is\n",
    "\n",
    "$$\\mu_{0j} = \\frac{\\sum_{i=1}^m x_{ij}}{m} $$\n",
    "\n",
    "This is how the normal distribution looks like\n",
    "\n",
    "<center><img src=\"../../../figures/normal.png\" width=400/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive classification assumes all features are independent, thus the total likelihood is just the product:\n",
    "$$P(x \\mid y) = \\prod_{i=1}^n P( x_i \\mid y )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, do $P(y)P(x|y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict based on which one is bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "1. Prepare your data\n",
    "    - $\\mathbf{X}$ and $\\mathbf{y}$ in the right shape\n",
    "        - $\\mathbf{X}$ -> $(m, n)$\n",
    "        - $\\mathbf{y}$ -> $(m,  )$\n",
    "        - Note that theta is not needed.  Why?\n",
    "    - train-test split\n",
    "    - feature scale\n",
    "    - clean out any missing data\n",
    "    - (optional) feature engineering\n",
    "2. Calculate the mean and std of each feature for each class (from the X_train). \n",
    "    $$\\mu_{0j} = \\frac{\\sum_{i=1}^m x_{ij}}{m} $$\n",
    "   The shape of your mean and std will be $(k, n)$\n",
    "3. Calculate the **likelihoods** of each sample of each feature (for X_test) using\n",
    "\n",
    "    $$ P(x \\mid y=1 ; \\mu_1, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_1)^{2}}{2\\sigma^{2}}}$$\n",
    "    $$ P(x \\mid y=0 ; \\mu_0, \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e ^{-\\frac{(x-\\mu_0)^{2}}{2\\sigma^{2}}}$$\n",
    "    \n",
    "    - The shape of likelihood for class 0 will be $(m, n)$\n",
    "    - Total likelihood is the product as follows:\n",
    "    \n",
    "    $$p(x \\mid y) = \\prod_{i=1}^n p(x_i \\mid y)$$\n",
    "    \n",
    "    - The shape of this total likelihood for class 0 will be $(m, )$\n",
    "    \n",
    "4. Find **priors** P(y)\n",
    "$$P(y = 1) = \\frac{\\Sigma_{i=1}^m 1(y=1)}{m}$$\n",
    "$$P(y = 0) = \\frac{\\Sigma_{i=1}^m 1(y=0)}{m}$$\n",
    "\n",
    "    - The shape of priors for class 0 will be simply a scalar\n",
    "\n",
    "5. Multiply $P(y)P(x \\mid y)$ for each class which will give us $p(y \\mid x)$ (**posteriors**)\n",
    "    \n",
    "    - For each class, the result of this is simply a multiplication between scalar and $(m, )$ resulting in a shape of $(m, )$, and you will have $k$ of such result.\n",
    "\n",
    "6. Simply compare $P(y)P(x \\mid y)$ for each class, whichever is bigger wins.  Note that we can ignore $P(x)$ since they can be canceled on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Prepare your data\n",
    "\n",
    "#### 1.1 Get your X and y in the right shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Feature scale your data to reach faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train test split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate the mean and std for each feature for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define the probability density function so we can later calculate $p(x \\mid y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the likelihood by calculating the probability density of each class $p(x \\mid y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Calculate thetotal likelihood by calculating the product of $p(x \\mid y) = \\prod_{i=1}^n p(x = i \\mid y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate the prior $p(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Calculate the posterior $p(x \\mid y)p(y)$ for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, once we are able to code from scratch, we can turn to our sklearn so we don't need to implement from scratch from now.  Naive Bayes Gaussian is implemented in Scikit-Learn's ``sklearn.naive_bayes.GaussianNB`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use predict_proba to print out the actual probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Generate a 2 class data using sklearn, and use them on Gaussian Naive Classification.  Put them into class and calculate accuracy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
