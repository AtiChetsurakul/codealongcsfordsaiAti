{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Unsupervised Learning - Clustering - GMM\n",
    "\n",
    "### Readings:\n",
    "- [VANDER] Ch5\n",
    "- [HASTIE] Ch14.3\n",
    "- https://scikit-learn.org/stable/modules/clustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models\n",
    "\n",
    "K-means has the following limitations:\n",
    "\n",
    "- assume spherical (circular) shape\n",
    "- assume equal size clusters\n",
    "\n",
    "We can use Gaussian Mixture Models (GMM), another cluster algorithm, to address these limitations.  In a *Gaussian mixture model*, the likelihood of our data can be defined as:\n",
    "\n",
    "$$\\log p(X | \\theta) = \\sum\\limits_{i=1}^m \\log p(x^{(i)} | \\theta) = p(x^{(i)} | z^{(i)} = k) \\cdot p(z^{(i)} = k)$$\n",
    "\n",
    "Let $p(x^{(i)} | z^{(i)} = k) = \\mathcal{N}(x | \\mu_k, \\Sigma_k)$ and $p (z^{(i)} = k) = \\pi_k$, we get\n",
    "\n",
    "$$\\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\theta := \\{\\mu_k, \\Sigma_k, \\pi_k: k = 1, \\cdots, K\\}$$\n",
    "\n",
    "$$0 \\leq \\pi_k \\leq 1$$\n",
    "\n",
    "$$\\sum\\limits_{k=1}^K \\pi_k=1$$\n",
    "\n",
    "and $\\mathcal{N}$ as the multivariate Gaussian distribution, computed using:\n",
    "\n",
    "$$\\mathcal{N} (x | \\mu_k, \\Sigma_k)= \\frac{1}{(2\\pi)^\\frac{n}{2}\\lvert{\\Sigma_k}\\rvert^\\frac{1}{2}}\\exp(-\\frac{1}{2}(x -\\mu_k)^T\\Sigma_{k}^{-1}(x - \\mu_k))$$\n",
    "\n",
    "\n",
    "Similar to K-means: it uses an expectationâ€“maximization (EM) approach which qualitatively does the following:\n",
    "\n",
    "#### Step 1: Define k random clusters\n",
    "\n",
    "1. Define k clusters from k random number of gaussian distribution (instead of some random places)\n",
    "    - Specifically, for each cluster k, randomly initialize parameters mean $\\mu_k$, covariance $\\Sigma_k$, fraction per class $\\pi_k$ and responsiblities (likelihood) of each sample $r^{(i)}_{k}$ \n",
    "    \n",
    "Recall that gaussian distribution is parametrized by the mean $\\mu$ and the covariance $\\Sigma$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=9, cluster_std=[1.0, 3.5, 0.5], random_state=42)\n",
    "plt.scatter(X[:, 0], X[:, 1], c =y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: EM-step\n",
    "\n",
    "Repeat until converged:\n",
    "\n",
    "   1. *E-step*: for each sample, assign samples to nearest cluster using likelihood (called responsibilities) of gaussian distribution (instead of distance)\n",
    "       - Evaluate responsibilities $r^{(i)}_{k}$ for every data point $x^{(i)}$ using \n",
    "\n",
    "$$ r^{(i)}_{k} = \\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "   2. *M-step*: for each cluster, update the gaussian distribution of each cluster (instead of the mean only).\n",
    "        - Restimate parameters $\\pi_k, \\mu_k, \\Sigma_k$ using the current responsibilites $r^{(i)}_{k}$ from the E step.\n",
    "\n",
    "$$ \\mu_k = \\frac{1}{N_k} \\sum\\limits_{i=1}^{m}r^{(i)}_{k}x^{(i)}$$\n",
    "\n",
    "$$ \\Sigma{_k} = \\frac{1}{N_k} \\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T$$\n",
    "\n",
    "$$ \\pi_k = \\frac{N_k}{m}$$\n",
    "\n",
    "where $N_k$ is the total responsibility of the $k$th mixture component along all samples\n",
    "\n",
    "$$N_k = \\sum\\limits_{i=1}^{m}r^{(i)}_k$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof\n",
    "\n",
    "Assume $X = \\{x^{(i)}, \\cdots, x^{(m)}\\}$ are drawn form an unkown distribution $p(x)$.  Our objective is to find a good approximation of this unknown distribution by means of a GMM with $K$ mixture components.  We exploit our i.i.d (independently and identically distributed) assumption, which leads to the log-likelihood as\n",
    "\n",
    "$$\\log p(X | \\theta) = \\sum\\limits_{i=1}^m \\log p(x^{(i)} | \\theta) = \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Our objective function is to find $\\theta$ that maximize the log-likehood $\\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\max_\\theta \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Our \"normal\" procedure would be to comute the gradient $\\frac{d\\mathcal{L}}{d\\theta}$ of the log-likelihood with respect to the model parameters $\\theta$, set it to 0, and solve for $\\theta$, however, if you try this yourself at home, you will find that it is not possible to find the closed form.\n",
    "\n",
    "One way we can do turns out to be the EM algorithm, where the key idea is to update one model parameter at a time, while keeping the others fixed.\n",
    "\n",
    "Before we find the partial derivatives, let us introduce a quantity that will play a central role in this algorithm: **responsibilities**.\n",
    "\n",
    "We define the quantity\n",
    "\n",
    "$$ r^{(i)}_{k} = \\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "as the *responsibility* of the $k$th mixture component for the $i$th data point.  \n",
    "\n",
    "$r^{(i)}_{k}$ basically gives us $$ \\frac{\\text{Probability of $x^{(i)}$ belonging to cluster k}}{\\text{Probability of $x^{(i)}$ over all clusters}} $$\n",
    "\n",
    "The responsibility $r^{(i)}_{k}$ of the $k$th mixture component for data point $x^{(i)}$ is proportional to the likelihood of the mixture component given the data point.\n",
    "\n",
    "$$p(x^{(i)} | \\pi_k, \\mu_k, \\Sigma_k) = \\pi_k\\mathcal{N}(x^{(i)}|\\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Therefore, mixture components have a high responsibility for a data point when the data point could be a **plausible sample** from that mixture component.  Note that \n",
    "\n",
    "$$r^{(i)} = r^{(i)}_1, r^{(i)}_2, \\cdots, r^{(i)}_k \\in \\mathbb{R}^k$$\n",
    "\n",
    "is a normalized probability vector, i.e., for each sample $i$\n",
    "\n",
    "$$\\sum\\limits_{j=1}^{k}r^{(i)}_j = 1$$\n",
    "\n",
    "$$ r^{(i)}_j \\geq 0 $$\n",
    "\n",
    "Thus this probability vector distributes probability mass among the $K$ mixture components, and we can think of $r^{(ik)}$ as probability that $x^{(i)}$ has been generated by the $k$th mixture component.\n",
    "\n",
    "By summing all the total responsibility of the $k$th mixture component along all samples, we get $N_k$.\n",
    "\n",
    "$$N_k = \\sum\\limits_{i=1}^{m}r^{(i)}_k$$\n",
    "\n",
    "Note that this value does not necessarily equal to 1.\n",
    "\n",
    "**Updating the mean**\n",
    "\n",
    "The update of the mean parameters $\\mu_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\mu_k^{new} = \\frac{\\sum\\limits_{i=1}^{m}r^{(i)}_{k}x^{(i)}}{\\sum\\limits_{i=1}^{m}r^{(i)}_{k}}$$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "Any local optimum of a function exhibits the property that its gradient with respect to the parameters must vanish, i.e., setting its partial derivative to zero.\n",
    "\n",
    "We take a partial derivative of our objective function with respect to the mean parameters $\\mu_k, k=1, \\cdots, K$.  To simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\mu_k} = \\sum\\limits_{j=1}^K \\pi_j \\frac{\\partial \\mathcal{N}(x^{(i)} | \\mu_j, \\Sigma_j)}{\\partial \\mu_k} = \\pi_k \\frac{\\partial \\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)}{\\partial \\mu_k} = \\pi_k(x^{(i)} - \\mu_k)^T \\Sigma_k^{-1}\\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "Now, taking all samples and log, since we know the partial derivative of $\\log$ something is $\\frac{1}{x}$, thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} =\\sum\\limits_{i=1}^{m} \\frac{\\partial \\log p(x^{(i)} | \\theta)}{\\partial \\mu_k} = \\sum\\limits_{i=1}^{m} \\frac{1}{p(x^{(i)} | \\theta)} \\frac{\\partial p(x^{(i)} | \\theta) }{\\partial \\mu_k} = \\sum\\limits_{i=1}^{m}(x^{(i)} - \\mu_k)^T \\Sigma_k^{-1}\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "To simplify, we can substitute $r^{(i)}_{k}$ into the equation, thus\n",
    "\n",
    "$$= \\sum\\limits_{i=1}^{m} r^{(i)}_{k}(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}$$\n",
    "\n",
    "We can now solve for $\\mu_k$ so that $\\frac{\\partial \\mathcal{L}}{\\partial \\mu_k} = 0$ and obtain\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{k}(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1} = 0$$\n",
    "\n",
    "Multiply both sides by $\\Sigma$ will cancel out the inverse $\\Sigma$, and move $\\mu_k$ to another side\n",
    "\n",
    "$$\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)}  = \\sum\\limits_{i=1}^{m} r^{(i)}_{k}\\mu_k$$\n",
    "\n",
    "$$\\frac{\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)} }{\\sum\\limits_{i=1}^{m} r^{(i)}_{k}}  = \\mu_k$$\n",
    "\n",
    "We can further substitute $N_k$ so that\n",
    "\n",
    "$$\n",
    "\\frac{1}{N_k}\\sum\\limits_{i=1}^{m} r^{(i)}_{k}x^{(i)} = \\mu_k\n",
    "$$\n",
    "\n",
    "Here we can interpret that $\\mu_k$ is pulled toward a data point $x^{(i)}$ with strength given by $r^{(i)}_{k}$.  The means are pulled stronger toward data points for which the corresponding mixture component has a high responsibility, i.e., a high likelihood.\n",
    "\n",
    "**Updating the covariances**\n",
    "\n",
    "The update of the covariance parameters $\\Sigma_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\Sigma_k^{new} = \\frac{1}{N_k} \\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T $$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "We take a partial derivative of our objective function with respect to the Sigma parameters $\\Sigma_k, k=1, \\cdots, K$.  Similarly, to simplify things, let's perform partial derivative without the log first and only consider one sample.\n",
    "\n",
    "$$\n",
    "\t\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k} = \\frac{\\partial}{\\partial \\Sigma_k} \\big(\\pi_k(2\\pi)^{-\\frac{D}{2}} \\det(\\Sigma_k)^{\\frac{1}{2}}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)\\big)\\big)\n",
    "$$\n",
    "\n",
    "Using derivative multiplication rule, we got\n",
    "\n",
    "$$\n",
    "= \\pi_k(2\\pi)^{-\\frac{D}{2}}\\big[\\frac{\\partial}{\\partial \\Sigma_k}\\det(\\Sigma_k)^{-\\frac{1}{2}}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)) + \\det(\\Sigma_k)^{-\\frac{1}{2}}\\frac{\\partial}{\\partial \\Sigma_k}exp\\big(-\\frac{1}{2}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k)\\big]\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial X}\\det(f(x)) = \\det(f(x))tr\\big(f(x)^{-1}\\frac{\\partial f(x)}{\\partial x}\\big)\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma_k}\\det(\\Sigma_k)^{-\\frac{1}{2}} = -\\frac{1}{2}\\det(\\Sigma_k)^{-\\frac{1}{2}}\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Using this following rule\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a^TXb}{\\partial X} = ab^T\n",
    "$$\n",
    "\n",
    "We get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\Sigma_k}(x^{(i)} - \\mu_k)^T\\Sigma^{-1}_k(x^{(i)} - \\mu_k) = -\\Sigma_k^{-1}(x^{(i)} - \\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Putting them together, we got:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k} = \\pi_k\\mathcal{N}(x^{(i)} | \\mu_k, \\Sigma_k) * \\big[-\\frac{1}{2}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1}\\big]\n",
    "$$\n",
    "\n",
    "Now consider all samples and log as well, the partial derivative of the log-likelihood with respect to $\\Sigma_k$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\Sigma_k} &=  \\sum\\limits_{i=1}^{m}\\frac{\\partial \\log p(x^{(i)} | \\theta)}{\\partial \\Sigma_k}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{1}{(p(x^{(i)} | \\theta)}\\frac{\\partial p(x^{(i)} | \\theta)}{\\partial \\Sigma_k}\\\\\n",
    "&=\\sum\\limits_{i=1}^{m}\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} * \\big[-\\frac{1}{2}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1})\\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting $r^{(i)}_{k}$, we got\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{2}\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(\\Sigma_k^{-1}-\\Sigma_k^{-1}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\Sigma_k^{-1})\\\\\n",
    "= -\\frac{1}{2}\\Sigma_k^{-1}\\sum\\limits_{i=1}^{m}r^{(i)}_{k} + \\frac{1}{2}\\Sigma_k^{-1}\\big(\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\big)\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "Setting this to zero, we obtain:\n",
    "\n",
    "$$\n",
    "N_k\\Sigma_k^{-1} = \\Sigma_k^{-1}\\big(\\sum\\limits_{i=1}^m r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\\big)\\Sigma_k^{-1}\n",
    "$$\n",
    "\n",
    "By solving for $\\Sigma_k$ we got\n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k}\\sum\\limits_{i=1}^{m}r^{(i)}_{k}(x^{(i)}-\\mu_k)(x^{(i)} - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "**Updating the pi - weight of mixture components**\n",
    "\n",
    "The update of the mixture weights $\\pi_k, k=1,\\cdots,K$ of the GMM is given by:\n",
    "\n",
    "$$ \\pi_k^{new} = \\frac{N_k}{m}$$\n",
    "\n",
    "To prove this:\n",
    "\n",
    "To find the partial derivative, we account for the equality constraint \n",
    "\n",
    "$$\\sum\\limits_{k=1}^K \\pi_k=1$$\n",
    "\n",
    "The Lagrangian $\\mathscr{L}$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathscr{L} &= \\mathcal{L} + \\beta\\big(\\sum\\limits_{k=1}^K \\pi_k-1\\big)\\\\\n",
    "&= \\sum\\limits_{i=1}^m \\log \\sum\\limits_{k=1}^K \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k) + \\beta\\big(\\sum\\limits_{k=1}^K \\pi_k-1\\big)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\pi_k$ as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\pi_k} &= \\sum\\limits_{i=1}^m\n",
    "\\frac{\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} + \\beta \\\\\n",
    "&= \\frac{1}{\\pi_k}\\sum\\limits_{i=1}^m\\frac{\\pi_k\\mathcal{N}(x^{(i)} \\mid \\mu_k, \\Sigma_{k})}{\\Sigma_{j=1}^{K} \\pi_j\\mathcal{N}(x^{(i)} \\mid \\mu_j, \\Sigma_j)} + \\beta\\\\\n",
    "&= \\frac{N_k}{\\pi_k} + \\beta\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the partial derivative with respect to $\\beta$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\beta} = \\sum\\limits_{i=1}^{K} \\pi_k - 1\n",
    "$$\n",
    "\n",
    "Setting both partial derivatives to zero yield\n",
    "\n",
    "$$\n",
    "\\pi_k = -\\frac{N_k}{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "1 = \\sum\\limits_{i=1}^K\\pi_k\n",
    "$$\n",
    "\n",
    "Using the top formula to solve for the bottom formula:\n",
    "\n",
    "$$    \n",
    "- \\sum\\limits_{i=1}^{m}\\frac{N_k}{\\beta} = 1\\\\\n",
    "= -\\frac{m}{\\beta} = 1\\\\\n",
    "= \\beta = -m\n",
    "$$\n",
    "\n",
    "Substitute $-m$ for $\\beta$ yield\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for *density estimation*.  That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to fit this with a two-component GMM viewed as a clustering model, the results are not particularly useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall *distribution* of the input data.\n",
    "This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input.\n",
    "For example, here are 400 new points drawn from this 16-component GMM fit to our original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data is a \"generated\" data - not real data.  So by understanding the distribution, it is useful to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many components for understanding distribution?\n",
    "\n",
    "The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset.\n",
    "A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the *likelihood* of the data under the model, using cross-validation to avoid over-fitting.\n",
    "\n",
    "Another means of correcting for over-fitting is to adjust the model likelihoods using some analytic criterion such as the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) or the [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion)\n",
    "\n",
    "$$ AIC = 2k - 2ln(L) $$\n",
    "\n",
    "$$ BIC = kln(n) - 2ln(L) $$\n",
    "\n",
    "where k is the number of features and L is maximum likelihood.  Basically,  number of features will increase AIC, while L will decrease AIC.  We want to good balance between model complexity and goodness of fit, thus lower the AIC, the better.\n",
    "\n",
    "Scikit-Learn's ``GMM`` estimator actually includes built-in methods that compute AIc, and so it is very easy to operate on this approach.\n",
    "\n",
    "Let's look at the AIC as a function as the number of GMM components for our moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal number of clusters is the value that minimizes the AIC or BIC, depending on which approximation we wish to use. The AIC tells us that our choice of 16 components above was probably too many: around 8-12 components would have been a better choice.\n",
    "As is typical with this sort of problem, the BIC recommends a simpler model.\n",
    "\n",
    "Notice the important point: this choice of number of components measures how well GMM works *as a density estimator*, not how well it works *as a clustering algorithm*.\n",
    "I'd encourage you to think of GMM primarily as a density estimator, and use it for clustering only when warranted within simple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: GMM for Generating New Data\n",
    "\n",
    "Here, since GMM is kind of model that tries to understand the data probability distribution.  GMM can be used to generate *new handwritten digits* from the standard digits corpus that we have used before.\n",
    "\n",
    "To start with, let's load the digits data using Scikit-Learn's data tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top of these to generate more.\n",
    "GMMs can have difficulty converging in such a high dimensional space, so we will start with an invertible dimensionality reduction algorithm on the data.\n",
    "Here we will use a straightforward PCA, asking it to preserve 99% of the variance in the projected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 41 dimensions, a reduction of nearly 1/3 with almost no information loss.\n",
    "Given this projected data, let's use the AIC to get a gauge for the number of GMM components we should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that around 110 components minimizes the AIC; we will use this model.\n",
    "Let's quickly fit this to the data and confirm that it has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can draw samples of 100 new points within this 41-dimensional projected space, using the GMM as a generative model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally*, we can use the inverse transform of the PCA object to construct the new digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what we've done here: given a sampling of handwritten digits, we have modeled the distribution of that data in such a way that we can generate brand new samples of digits from the data: these are \"handwritten digits\" which do not individually appear in the original dataset, but rather capture the general features of the input data as modeled by the mixture model.\n",
    "Such a generative model of digits is perhaps the basic idea behind **\"Generative Adversarial Network\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use GMM\n",
    "\n",
    "Pros:\n",
    "    - Address the limitations of K-means\n",
    "    - Can be used to generate data, since we know $P(x | y)$\n",
    "\n",
    "Cons:\n",
    "    - Just like K-mean, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ===Task===\n",
    "\n",
    "Your work: Let's modify the above scratch code:\n",
    "- Modify so it performs early stopping when the log likelihood does not improve anymore.\n",
    "- Perform plotting every 5 iterations on the resulting clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
