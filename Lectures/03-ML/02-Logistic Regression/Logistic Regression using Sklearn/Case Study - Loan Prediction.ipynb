{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Case Study - Loan Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we will be working on a dataset called the Loan Prediction dataset.\n",
    "\n",
    "This dataset concern the data of loan application and the result whether if the loan was approved or not.\n",
    "\n",
    "We are given 2 set, the training set and the test set.\n",
    "\n",
    "The training set contains 614 samples and 13 features, 12 of which are the independent variables and the last feature `Loan_Status` is the dependent variable.\n",
    "\n",
    "The test set contains 367 samples with the same 12 features but without the `Loan_Status` columns. So it will be representing the unseen data that we will be implementing our model on.\n",
    "\n",
    "Our goal is to do analyze the data to understand this problem and create a classification model for predicting the `Loan_Status`\n",
    "\n",
    "The first thing to do is to clean the data, by filling in missing values and converting categorical data to real numbers. We will use the Python libraries pandas and sklearn to help with the data cleaning and preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tasks will be divided into 2 parts\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "    * Load and view the Dataset\n",
    "    * Are there any null values ? How will you wrangle/handle them?\n",
    "    * Are there any outliers values ? How will you wrangle/handle them?\n",
    "    * Do you notice any patterns or anomalies in the data? Can you plot them?\n",
    "2. Statistical Analysis\n",
    "    * Training a machine learning model for Loan prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Exploratory Data Analysis\n",
    "\n",
    "* Load and view the dataset\n",
    "* Are there any null values or outliers? How will you wrangle/handle them?\n",
    "* Do you notice any patterns or anomalies in the data? Can you plot them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Load and view the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_LoanPrediction.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b8f7cfa56006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 'data/test_LoanPrediction.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 'data/test_LoanPrediction.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train_LoanPrediction.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test_LoanPrediction.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DSAI/Environments/teaching_env/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_LoanPrediction.csv'"
     ]
    }
   ],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import the 2 dataset from these paths\n",
    "# 'data/test_LoanPrediction.csv'\n",
    "# 'data/test_LoanPrediction.csv'\n",
    "train_data = pd.read_csv('data/train_LoanPrediction.csv')\n",
    "test_data  = pd.read_csv('data/test_LoanPrediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of Training and Test set\n",
    "print('Training data shape : ', train_data.shape)\n",
    "print('Test data shape     : ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the \"head\" of the training set\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the \"info\" of the training set\n",
    "# Notice that this will tell us the  Non-null counts and the dtypes of each colum\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Are there any null values? How will you handle them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check for missing values in the Training and Test set again with isnull()\n",
    "print('Missing values in Train data : \\n', train_data.isnull().sum() )\n",
    "print(\"=\"*30)\n",
    "print('Missing values in Test data : \\n', test_data.isnull().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling categorical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's first focus on the `Married` column. We can see from the table above that the `Married` has 3 missing values in the training set and 0 missing values in the test set so let's fill the training set!\n",
    "\n",
    "We will use the distribution over the train dataset then fill in the missing values in approximately the same ratio with fillna()\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "   * Compute ratio of each category value in the training set\n",
    "   * Divide the missing values based on ratio\n",
    "   * Fill in the missing values according to the ratio\n",
    "   * Don't forget to print the values before and after filling the missing values for confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the values in 'Married' columns.\n",
    "print(train_data['Married'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ratio of each category value in the training set\n",
    "married = train_data['Married'].value_counts()\n",
    "\n",
    "ratio_married = married[0] / sum(married.values)\n",
    "ratio_not_married = married[1] / sum(married.values)\n",
    "\n",
    "print('Numner of unique elements in Married variable : ', married.shape)\n",
    "print('Ratio of Married to all     : ', ratio_married)\n",
    "print('Ratio of Not Married to all : ', ratio_not_married)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the missing values based on ratio\n",
    "yes_num_train = round(ratio_married * train_data['Married'].isnull().sum())\n",
    "no_num_train  = round(ratio_not_married * train_data['Married'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values according to the ratio\n",
    "# Hint : use the parameter called 'limit' in fillna()\n",
    "train_data['Married'].fillna('Yes', inplace = True, limit = yes_num_train)\n",
    "train_data['Married'].fillna('No', inplace = True, limit = no_num_train)  \n",
    "\n",
    "# Check if all missing data were filled\n",
    "print(train_data['Married'].value_counts()) \n",
    "print('Missing values in Train data : \\n', train_data.isnull().sum() )\n",
    "print(\"=\"*30)\n",
    "print('Missing values in Test data : \\n', test_data.isnull().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the number of missing values in the `Married` attribute is 0. We have successfully filled the `Married` column!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still have to fill the following categorical columns :\n",
    "        \n",
    "        - Gender\n",
    "        - Dependents\n",
    "        - Self_Employed\n",
    "        - Loan_Amount_Term\n",
    "        - Credit_History\n",
    "        \n",
    "For some of them we need to fill in both the training and test set!\n",
    "\n",
    "So let's write a function that can calculate the ratio from the training data and fill the missing values accordingly.\n",
    "* Notice that we will use the distribution from the TRAINING set to fill in both the training and test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your function here\n",
    "def fill_data_with_ratio(train_data, fill_data_1, fill_data_2, column_name):\n",
    "    \n",
    "    count_column = train_data[column_name].value_counts()\n",
    "    value_list = list(count_column.index)\n",
    "    \n",
    "    limits_1 = []\n",
    "    limits_2 = []\n",
    "    \n",
    "    for value in value_list:\n",
    "        ratio = count_column[value] / sum(count_column.values)\n",
    "        limits_1.append(round(ratio * fill_data_1[column_name].isnull().sum()))\n",
    "        if any(fill_data_2):\n",
    "            limits_2.append(round(ratio * fill_data_2[column_name].isnull().sum()))\n",
    "    \n",
    "    for id_, limit in enumerate(limits_1):\n",
    "        if limit == 0 :\n",
    "            limit = 1\n",
    "        fill_data_1[column_name].fillna(value_list[id_], inplace = True, limit = limit)\n",
    "        \n",
    "    if any(fill_data_2):\n",
    "        for id_, limit in enumerate(limits_2):\n",
    "            if limit == 0 :\n",
    "                limit = 1\n",
    "            fill_data_2[column_name].fillna(value_list[id_], inplace = True, limit = limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function with the 'Gender' column !\n",
    "# This columm has missing values in both the training and test set.\n",
    "\n",
    "# Count the values of 'Gender' Before filling\n",
    "print(\"========== BEFORE ==========\")\n",
    "print(train_data['Gender'].value_counts())\n",
    "print(test_data['Gender'].value_counts())\n",
    "\n",
    "fill_data_with_ratio(train_data, train_data, test_data, 'Gender')\n",
    "\n",
    "# Count the values of 'Gender' after filling\n",
    "print(\"========== AFTER ==========\")\n",
    "print(train_data['Gender'].value_counts())\n",
    "print(test_data['Gender'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat this step for all categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function to fill the 'Dependents' column !\n",
    "# This columm has missing values in both the training and test set.\n",
    "\n",
    "# Count the values of 'Dependents' Before filling\n",
    "print(\"========== BEFORE ==========\")\n",
    "print(train_data['Dependents'].value_counts())\n",
    "print(test_data['Dependents'].value_counts())\n",
    "\n",
    "# Use your finction here\n",
    "fill_data_with_ratio(train_data, train_data, test_data, 'Dependents')\n",
    "\n",
    "# Count the values of 'Dependents' after filling\n",
    "print(\"========== AFTER ==========\")\n",
    "print(train_data['Dependents'].value_counts())\n",
    "print(test_data['Dependents'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that we have value 0, 1, 2 ,3+\n",
    "# Let's convert category value \"3+\" to \"4\"\n",
    "# so that we can convert them to int and it will be easier for the model later\n",
    "\n",
    "# 'replace' value '3+' with '4'\n",
    "train_data['Dependents'].replace('3+', '4', inplace = True)\n",
    "test_data['Dependents'].replace('3+', '4', inplace = True)\n",
    "\n",
    "# Notice that the values are still of type string, we will fix this later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function with the 'Self_Employed' column !\n",
    "# This columm also has missing values in both the training and test set.\n",
    "\n",
    "# Count the values of 'Self_Employed' Before filling\n",
    "print(\"========== BEFORE ==========\")\n",
    "print(train_data['Self_Employed'].value_counts())\n",
    "print(test_data['Self_Employed'].value_counts())\n",
    "\n",
    "fill_data_with_ratio(train_data, train_data, test_data, 'Self_Employed')\n",
    "\n",
    "# Count the values of 'Self_Employed' after filling\n",
    "print(\"========== AFTER ==========\")\n",
    "print(train_data['Self_Employed'].value_counts())\n",
    "print(test_data['Self_Employed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function with the 'Loan_Amount_Term' column !\n",
    "# This columm also has missing values in both the training and test set.\n",
    "\n",
    "# Count the values of 'Loan_Amount_Term' Before filling\n",
    "print(\"========== BEFORE ==========\")\n",
    "print(train_data['Loan_Amount_Term'].value_counts())\n",
    "print(test_data['Loan_Amount_Term'].value_counts())\n",
    "\n",
    "fill_data_with_ratio(train_data, train_data, test_data, 'Loan_Amount_Term')\n",
    "\n",
    "# Count the values of 'Loan_Amount_Term' after filling\n",
    "print(\"========== AFTER ==========\")\n",
    "print(train_data['Loan_Amount_Term'].value_counts())\n",
    "print(test_data['Loan_Amount_Term'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function with the 'Credit_History' column !\n",
    "# This columm also has missing values in both the training and test set.\n",
    "\n",
    "# Count the values of 'Credit_History' Before filling\n",
    "print(\"========== BEFORE ==========\")\n",
    "print(train_data['Credit_History'].value_counts())\n",
    "print(test_data['Credit_History'].value_counts())\n",
    "\n",
    "fill_data_with_ratio(train_data, train_data, test_data, 'Credit_History')\n",
    "\n",
    "# Count the values of 'Credit_History' after filling\n",
    "print(\"========== AFTER ==========\")\n",
    "print(train_data['Credit_History'].value_counts())\n",
    "print(test_data['Credit_History'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Numerical Values\n",
    "Finally, `LoanAmount` still has some missing values.\n",
    "This column contains numeric attribute.\n",
    "We should check the distribution of the data before deciding how to fill them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram to see the data distribution of 'LoanAmount'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.displot(train_data['LoanAmount'])\n",
    "plt.title('LoanAmount distribution', size=16)\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also try plot a box plot\n",
    "train_data['LoanAmount'].plot(kind='box', figsize=(3,4), patch_artist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers so we will be using the median over the training set to fill the missing values to avoid effects of outliers on the center value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the median of the Training set to fill both the training and test set.\n",
    "train_data['LoanAmount'].fillna((train_data['LoanAmount'].median()), inplace=True)\n",
    "test_data['LoanAmount'].fillna((train_data['LoanAmount'].median()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values in every column again, there shoul be no missing data left now\n",
    "print('Missing values in Train data : \\n', train_data.isnull().sum() )\n",
    "print(\"=\"*30)\n",
    "print('Missing values in Test data : \\n', test_data.isnull().sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Are there any outlier ? How will you handle them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the outliers of all numerical columns \n",
    "\n",
    "# select columns to plot\n",
    "import numpy as np\n",
    "df_to_plot = train_data.drop(columns=['Loan_ID','Loan_Amount_Term','Credit_History']).select_dtypes(include=np.number)\n",
    "\n",
    "# make a subplot out of df_to_plot and plot the box plots\n",
    "df_to_plot.plot(subplots=True, layout=(4,4), kind='box', figsize=(12,14), patch_artist=True)\n",
    "plt.subplots_adjust(wspace=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple features contain outliers, but the nothing indicate data entry errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Do you notice any patterns or anomalies in the data? Can you plot them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Plots for categorical data\n",
    "Let's do distribution plot to see how many people in each category applied for the loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# plot a distribution plot of 'Gender'\n",
    "sns.displot(train_data['Gender'])\n",
    "plt.title('Gender', size=16)\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Married'\n",
    "sns.displot(train_data['Married'])\n",
    "plt.title('Married', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Dependents'\n",
    "sns.displot(train_data['Dependents'])\n",
    "plt.title('Dependents', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Education'\n",
    "sns.displot(train_data['Education'])\n",
    "plt.title('Education', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Self_Employed'\n",
    "sns.displot(train_data['Self_Employed'])\n",
    "plt.title('Self_Employed', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Credit_History'\n",
    "sns.displot(train_data['Credit_History'])\n",
    "plt.title('Credit_History', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'Property_Area'\n",
    "sns.displot(train_data['Property_Area'])\n",
    "plt.title('Property_Area', size=16)\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Plots for Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'ApplicantIncome'\n",
    "sns.displot(train_data[train_data['ApplicantIncome']<20000]['ApplicantIncome'])\n",
    "plt.title('ApplicantIncome distribution', size=16)\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'CoapplicantIncome'\n",
    "sns.displot(train_data[train_data['CoapplicantIncome']<10000]['CoapplicantIncome'])\n",
    "plt.title('CoapplicantIncome distribution', size=16)\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a distribution plot of 'LoanAmount'\n",
    "sns.displot(train_data[train_data['LoanAmount']<10000]['LoanAmount'])\n",
    "plt.title('LoanAmount distribution', size=16)\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model between 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try plotting the data and regression model between the `ApplicantIncome` and `LoanAmount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='ApplicantIncome', y='LoanAmount', data=train_data[train_data['ApplicantIncome'] < 10000]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for `CoapplicantIncome` and `LoanAmount`\n",
    "sns.lmplot(x='CoapplicantIncome', y='LoanAmount', data=train_data[train_data['CoapplicantIncome'] < 6000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some box plots to see the relationships between some of the attributes with `LoanAmount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot with x as 'Dependents' and y as 'LoanAmount'\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.boxplot(x='Dependents', y='LoanAmount', data=train_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot with x as 'Education' and y as 'LoanAmount'\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.boxplot(x='Education', y='LoanAmount', data=train_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot with x as 'Property_Area' and y as 'LoanAmount'\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.boxplot(x='Property_Area', y='LoanAmount', data=train_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot with x as 'Married' and y as 'LoanAmount'\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.boxplot(x='Married', y='LoanAmount', data=train_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a boxplot with x as 'Self_Employed' and y as 'LoanAmount'\n",
    "plt.figure(figsize=(4,4))\n",
    "sns.boxplot(x='Self_Employed', y='LoanAmount', data=train_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Statistical Analysis\n",
    "In this workshop we will only do one type of statistical analysis\n",
    "   * Training a machine learning model for Loan prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model to decide if we should approve or reject the loan.\n",
    "\n",
    "The independent variables or X will be:\n",
    "    \n",
    "    - Gender\n",
    "    - Married\n",
    "    - Dependents\n",
    "    - Education\n",
    "    - Self_Employed\n",
    "    - ApplicantIncome\n",
    "    - CoapplicantIncome\n",
    "    - LoanAmount\n",
    "    - Loan_Amount_Term\n",
    "    - Credit_History\n",
    "    - Property_Area\n",
    "    \n",
    "These will be used to predict the dependent variable or y which is `Loan_status`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will convert all values to type int or float so that the model can process them, as we can see here that some columns still have `object` as dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.info())\n",
    "print(\"=\"*50)\n",
    "print(test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will drop the `Loan_ID` column as it will not help with learning of the model.\n",
    "train_data.drop(columns=['Loan_ID'], inplace=True)\n",
    "test_data.drop(columns=['Loan_ID'], inplace=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace string with int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's convert the `Gender` column to 0.0 for Male and 1.0 for Female\n",
    "train_data['Gender'].replace('Male', 0.0, inplace = True)\n",
    "train_data['Gender'].replace('Female', 1.0, inplace = True)\n",
    "\n",
    "test_data['Gender'].replace('Male', 0.0, inplace = True)\n",
    "test_data['Gender'].replace('Female', 1.0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this step for all of the categorical columns!\n",
    "\n",
    "There's a lot more columns to do!\n",
    "So you might want to use that 'factorize' function !\n",
    "(https://pandas.pydata.org/docs/reference/api/pandas.factorize.html)\n",
    "\n",
    "or we can just replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Married' to int\n",
    "train_married = pd.Categorical(list(train_data['Married']), categories=['No','Yes'])\n",
    "test_married = pd.Categorical(list(test_data['Married']), categories=['No','Yes'])\n",
    "\n",
    "train_codes, uniques = pd.factorize(train_married,sort=True)\n",
    "train_data['Married'] = train_codes\n",
    "\n",
    "test_codes, uniques = pd.factorize(test_married, sort=True)\n",
    "test_data['Married'] = test_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Education' to int\n",
    "train_education = pd.Categorical(list(train_data['Education']), categories=['Not Graduate','Graduate'])\n",
    "test_education = pd.Categorical(list(test_data['Education']), categories=['Not Graduate','Graduate'])\n",
    "\n",
    "train_codes, uniques = pd.factorize(train_education,sort=True)\n",
    "train_data['Education'] = train_codes\n",
    "\n",
    "test_codes, uniques = pd.factorize(test_education, sort=True)\n",
    "test_data['Education'] = test_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Self_Employed' to int\n",
    "train_selfem = pd.Categorical(list(train_data['Self_Employed']), categories=['No','Yes'])\n",
    "test_selfem = pd.Categorical(list(test_data['Self_Employed']), categories=['No','Yes'])\n",
    "\n",
    "train_codes, uniques = pd.factorize(train_selfem,sort=True)\n",
    "train_data['Self_Employed'] = train_codes\n",
    "\n",
    "test_codes, uniques = pd.factorize(test_selfem, sort=True)\n",
    "test_data['Self_Employed'] = test_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Property_Area' to int\n",
    "train_prop = pd.Categorical(list(train_data['Property_Area']), categories=['Rural','Semiurban','Urban'])\n",
    "test_prop = pd.Categorical(list(test_data['Property_Area']), categories=['Rural','Semiurban','Urban'])\n",
    "\n",
    "train_codes, uniques = pd.factorize(train_prop,sort=True)\n",
    "train_data['Property_Area'] = train_codes\n",
    "\n",
    "test_codes, uniques = pd.factorize(test_prop, sort=True)\n",
    "test_data['Property_Area'] = test_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Loan_Status' to int\n",
    "train_status = pd.Categorical(list(train_data['Loan_Status']), categories=['N','Y'])\n",
    "\n",
    "train_codes, uniques = pd.factorize(train_status,sort=True)\n",
    "train_data['Loan_Status'] = train_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values in 'Dependents' to int\n",
    "train_data['Dependents'] = train_data['Dependents'].astype(int)\n",
    "test_data['Dependents'] = test_data['Dependents'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see which columns are still not float or int\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All data is either float or int now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Training a machine learning model for Loan prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would want to split the train data into train and validation set first. \n",
    "Then use the training and validation set for training and validation of the model.\n",
    "\n",
    "You can use any classification model from sklearn to classify `Loan_Status`. \n",
    "Experiment with various hyperparameters, you may use Cross-Validation, Validation curve , Learning Curve or GridSearch as you want but dont forget to get the validation accuracy and the best model.\n",
    "\n",
    "Then, use your best model to predict `Loan_status` of the test set.\n",
    "(we cannot calculate the accuracy on the test data because we don't have the ground truth (or the real values of 'Loan_Status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y data for a Machine Learning Model (or any other classification model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which shape sklearn needs for Logistic Regression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shape of train and test data\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data has 1 column more than test_data. That colum is the `Loan_Status` column that we want to predict (or our y) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert train_data and test_data to X_train, y_train and X_test\n",
    "X = train_data.iloc[:,:-1].to_numpy()\n",
    "print(X.shape)\n",
    "\n",
    "y = train_data.iloc[:,-1].to_numpy()\n",
    "print(y.shape)\n",
    "\n",
    "X_test = test_data.to_numpy()\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into X_train, X_val, y_train, y_val\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a classification model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training set\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the model accuracy on validation set\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the best model to predict the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
