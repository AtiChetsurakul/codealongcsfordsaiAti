{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression from Scratch\n",
    "\n",
    "| | Egg price  | Gold price    | Oil price   | GDP   |\n",
    "|---:|:-------------|:-----------|:------|:------|\n",
    "| 1 | 3  | 100       | 4   | 21   |\n",
    "| 2 | 4  | 500    | 7   | 43     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sample 1  $x^1$\n",
    "x1 = np.array([3, 100, 4])\n",
    "y1 = np.array([21])\n",
    "\n",
    "#what's the idea of prediction?  What is machine learning?\n",
    "#- find the weights that can bring you from x1 to y1\n",
    "\n",
    "#first sample\n",
    "#3 * w1 + 100 * w2 + 4 * w3 = 21\n",
    "#3 * 1  + 100 * 1  + 4 * 1  = 107\n",
    "#3 * 7  + 100 * 1  + 4 * -25  = 21\n",
    "\n",
    "#machine learning is trying to find the `best` weights\n",
    "\n",
    "#2nd sample\n",
    "#4 * w1 + 500 * w2 + 7 * w3   = 43\n",
    "#4 * 7  + 500 * 1  + 7 * -25  = 353 \n",
    "\n",
    "#machine learning is trying to find the `best` weights ACROSS all samples....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of terms and notations\n",
    "\n",
    "#2 samples\n",
    "#3 features - egg price, gold price, oil price\n",
    "    #features are the variables used for predicting the label\n",
    "    #factors, independent variables, predictors, X\n",
    "\n",
    "#egg price - x_1 --> always a vector,  e.g., [3, 4]\n",
    "#gold price - x_2 --> always a vector, e.g., [100, 500]\n",
    "#oil price - x_3 --> always a vector, e.g., [4, 7]\n",
    "#we call egg price + gold price + oil price - whole `feature matrix` --> \\mathbf{X}\n",
    "    \n",
    "#1 label - gdp\n",
    "    #label is the variable that we want to predict....\n",
    "    #target, outcome, y\n",
    "    #y_1 = y = a vector of labels, e.g., [21, 43]\n",
    "    \n",
    "#Tips: small and big\n",
    "# small mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math notations:\n",
    "\n",
    "- normal a -> scalar (one number)\n",
    "- bold  $\\mathbf{a}$  --> vector (a 1D numpy array)\n",
    "- bold  $\\mathbf{A}$  --> matrix (a 2D numpy array....)\n",
    "\n",
    "- $\\mathbf{x}_1^2$  --> feature 1, second sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How dot product works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([  [3, 100, 4] , [4, 500, 7]  ])\n",
    "X.shape  #(2, 3) means 2 samples = m, 3 features = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights = theta = params\n",
    "theta = np.array([7, 1, -25])\n",
    "theta.shape  #weights must be the sample shape as X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21, 353])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.dot(theta)\n",
    "#to be able to dot, the number should be same in the close pair\n",
    "#(2, 3)  @ (3, ) = (2, )\n",
    "#(4, 6)  @ (6, 1) = (4, 1)\n",
    "#(4, 6, 1) @ (1, 2) = (4, 6, 1, 2)\n",
    "X @ theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0] * theta[0] + X[0][1] * theta[1] + X[0][2] * theta[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for linear regression / gradient descent\n",
    "\n",
    "Step 1: Randomize your weight\n",
    "  - weight.shape (n, )\n",
    "\n",
    "Step 2: Use this inital weight to predict\n",
    "  - you will get errors\n",
    "\n",
    "Step 3: Find the derivative\n",
    "\n",
    "$\\mathbf{X}^\\top (\\mathbf{\\hat{y}} - \\mathbf{y})$\n",
    "\n",
    "Step 4: Change the weight\n",
    "\n",
    "$\\mathbf{w} = \\mathbf{w} - \\alpha * \\mathbf{X}^\\top (\\mathbf{\\hat{y}} - \\mathbf{y})$\n",
    "\n",
    "Step 5:  Repeat Step 2, 3, 4, until you either (1) reach the max iteration, or (2) your validation loss does not decrease anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load some toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "#print the shape of X and y\n",
    "X.shape, y.shape\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1\n",
    "\n",
    "#print one row of X, and maybe try to see what it is...\n",
    "#print one row of y, and maybe try to see what it is....\n",
    "# X[0]\n",
    "# y[0]\n",
    "# diabetes.feature_names\n",
    "# label is blood glucose level.....\n",
    "\n",
    "#please help me set m and \n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "#write an assert function to check that X and y has same amount of samples...\n",
    "assert m == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We skip EDA and cleaning, because we are lazy; but actually this dataset is already clean..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 9999\n",
    ")\n",
    "\n",
    "#assert that X_train and y_train have the same amount of samples\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "#assert that X_test and y_test have the same amount of samples\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#standardize the training set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "#standardize the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Add intercept to your X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: if your X is        [  [3, 2, 4],    [2, 6, 8]  ]\n",
    "# I want you to make it into   [  [1, 3, 2, 4], [1, 2, 6, 8]  ]\n",
    "# Why 1?  because imagine you have another weight, which let's call w0\n",
    "# this w0 is actually the intercept; so multiply with 1, will do nothing\n",
    "# so we can still use X @ theta....\n",
    "\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_train on axis=1, to add these ones to X_train\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_test on axis=1, to add these ones to X_test\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fitting!!! Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put everything fit()\n",
    "\n",
    "#1. randomize our theta\n",
    "#please help me create a random theta of size (X_train.shape[1], )\n",
    "theta = np.ones(X_train.shape[1])\n",
    "#why X_train.shape[1]\n",
    "\n",
    "#5. repeat 2, 3, 4\n",
    "#please put a for loop for 2, 3, 4, for 1000 times\n",
    "#set 1000 call it max_iter\n",
    "#for _ in range(max_iter):\n",
    "max_iter = 1000\n",
    "alpha = 0.0001\n",
    "\n",
    "def predict(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "def _grad(X, error):\n",
    "    return X.T @ error\n",
    "\n",
    "def fit(X_train, y_train, theta, max_iter, alpha):\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        #2. predict\n",
    "        yhat = predict(X_train, theta)  #put this into a function called predict(X_train, theta)\n",
    "\n",
    "        #2.1 can you guys compute the squared error\n",
    "        # squared_error = ((yhat - y_train) ** 2).sum()\n",
    "        #print the mean squared error, we can see whether MSE goes down eventually...\n",
    "        mse =  mean_squared_error(y_train, yhat)\n",
    "        if(i % 50 == 0):\n",
    "            print(f\"MSE: {mse}\")  \n",
    "\n",
    "        #3. get derivatives\n",
    "        deriv = _grad(X_train, yhat - y_train)\n",
    "\n",
    "        #4. update weight\n",
    "        theta = theta - alpha * deriv\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 28562.951824703876\n",
      "MSE: 3897.3289014795173\n",
      "MSE: 2877.3645633448773\n",
      "MSE: 2831.24639141041\n",
      "MSE: 2827.9023866215557\n",
      "MSE: 2826.5434217394395\n",
      "MSE: 2825.3251211991583\n",
      "MSE: 2824.1524285941555\n",
      "MSE: 2823.016722214682\n",
      "MSE: 2821.91531119463\n",
      "MSE: 2820.8463667746114\n",
      "MSE: 2819.8083642041256\n",
      "MSE: 2818.79996461671\n",
      "MSE: 2817.819969874501\n",
      "MSE: 2816.867295693387\n",
      "MSE: 2815.9409519375267\n",
      "MSE: 2815.040027131533\n",
      "MSE: 2814.163676031542\n",
      "MSE: 2813.311109582284\n",
      "MSE: 2812.481586776238\n"
     ]
    }
   ],
   "source": [
    "theta = fit(X_train, y_train, theta, max_iter, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3079.246779652193"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(X_test, theta)\n",
    "\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Normal_lr(object):\n",
    "\n",
    "    def __init__(self,max_iter = 3000,alpha = 0.001):\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        pass\n",
    "    def predict(self,X, theta):\n",
    "        return X @ theta\n",
    "\n",
    "    def cal_score(self,ytrue, ypred):\n",
    "        self.score = ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "        return self.score\n",
    "\n",
    "    def _grad(self,X, error):\n",
    "        return X.T @ error\n",
    "        \n",
    "    def _run_grad(self,X_method_train,y_method_train,theta):\n",
    "        yhat = self.predict(X_method_train, theta)\n",
    "        deriv = self._grad(X_method_train, yhat - y_method_train)\n",
    "\n",
    "        self.theta = self.theta - self.alpha * deriv\n",
    "\n",
    "    def fit(self,X_train, y_train,message =True):\n",
    "        self.theta = np.zeros(X_train.shape[1])\n",
    "        for i in range(self.max_iter):\n",
    "            #2. predict\n",
    "            yhat = self.predict(X_train, theta)  #put this into a function called predict(X_train, theta)\n",
    "\n",
    "            #2.1 can you guys compute the squared error\n",
    "            # squared_error = ((yhat - y_train) ** 2).sum()\n",
    "            #print the mean squared error, we can see whether MSE goes down eventually...\n",
    "            if message:\n",
    "                mse =  self.cal_score(y_train, yhat)\n",
    "                if(i % 50 == 0):\n",
    "                    print(f\"MSE: {mse}\")  \n",
    "\n",
    "            #3. get derivatives\n",
    "            deriv = self._grad(X_train, yhat - y_train)\n",
    "\n",
    "            #4. update weight\n",
    "            self.theta = self.theta - self.alpha * deriv\n",
    "        \n",
    "        self.coef = self.theta[1:]\n",
    "        self.bias = self.theta[:1]\n",
    "            \n",
    "        return self.bias,self.coef\n",
    "\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return self._score\n",
    "    @score.setter\n",
    "    def score(self,s):\n",
    "        self._score = s\n",
    "    @property\n",
    "    def coef(self):\n",
    "        return self._coef\n",
    "    @coef.setter\n",
    "    def coef(self,c):\n",
    "        self._coef = c\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self._bias\n",
    "    @bias.setter\n",
    "    def bias(self,b):\n",
    "        self._bias = b    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mini_batch(Normal_lr):\n",
    "\n",
    "    def fit(self,X_train, y_train,num_epochs = 50,batch_size=50,max_iter=1):\n",
    "        # i = 0\n",
    "        # j = 0\n",
    "        # k = 0\n",
    "        self.max_iter = max_iter\n",
    "        self.theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            permuted_index = np.random.permutation(X_train.shape[0])\n",
    "            Xran_train = X_train[permuted_index]\n",
    "            yran_train = y_train[permuted_index]\n",
    "            # k+= 1\n",
    "\n",
    "            for i in range(0,X_train.shape[0],batch_size):\n",
    "                X_mini_train = Xran_train[i:i+batch_size]\n",
    "                y_mini_train = yran_train[i:i+batch_size]\n",
    "                # j += 1\n",
    "                # print(f'{e}:{i}')\n",
    "\n",
    "                for j in range(self.max_iter):\n",
    "\n",
    "                    # i += 1\n",
    "                    yhat = self.predict(X_mini_train, theta)\n",
    "\n",
    "\n",
    "                    #3. get derivatives\n",
    "                    deriv = self._grad(X_mini_train, yhat - y_mini_train)\n",
    "\n",
    "                    #4. update weight\n",
    "                    self.theta = self.theta - self.alpha * deriv\n",
    "                    \n",
    "        # print(i,j,k)\n",
    "        self.coef = self.theta[1:]\n",
    "        self.bias = self.theta[:1]\n",
    "            \n",
    "        return self.bias,self.coef\n",
    "\n",
    "\n",
    "   ### CORE OF ML\n",
    "   # # VERY IMPORTANT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Three_fit(Normal_lr):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self,X_train, y_train,num_epochs = 50,batch_size=50,type = 'batch'):\n",
    "\n",
    "        self.theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            permuted_index = np.random.permutation(X_train.shape[0])\n",
    "            X_train = X_train[permuted_index]\n",
    "            y_train = y_train[permuted_index]\n",
    "\n",
    "            if type == 'normal':\n",
    "                X_method_train = X_train\n",
    "                y_method_train = y_train\n",
    "                self._run_grad(X_method_train,y_method_train,theta)\n",
    "                pass\n",
    "\n",
    "            elif type == 'batch':\n",
    "                for i in range(0,X_train.shape[0],batch_size):\n",
    "                    X_method_train = X_train[i:i+batch_size]\n",
    "                    y_method_train = y_train[i:i+batch_size]\n",
    "                    self._run_grad(X_method_train,y_method_train,theta)\n",
    "\n",
    "            elif type == 'sto':\n",
    "                for i in range(0,X_train.shape[0],1):\n",
    "                    X_method_train = X_train[i,:].reshape(1,-1)\n",
    "                    y_method_train = y_train[i]\n",
    "                    self._run_grad(X_method_train,y_method_train,theta)\n",
    "                pass\n",
    "            else:\n",
    "                raise NameError\n",
    "\n",
    "        self.coef = self.theta[1:]\n",
    "        self.bias = self.theta[:1]\n",
    "            \n",
    "        return self.bias,self.coef\n",
    "\n",
    "\n",
    "\n",
    "   ### CORE OF ML\n",
    "   # # VERY IMPORTANT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "thw = Three_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.53599805e-11]),\n",
       " array([-0.04248061, -0.06166244, -0.1167385 ,  0.03905905, -5.62396234,\n",
       "         4.26276798,  2.53323108,  1.03571006,  2.04680922,  0.01260244]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thw.fit(X_train, y_train,type = 'sto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = Ati_learn_lr()\n",
    "\n",
    "# lr.fit(X_train, y_train,message=False)\n",
    "# ati_hat = lr.predict(X_test,theta)\n",
    "# mseati = lr.cal_score(y_test, ati_hat)\n",
    "# print(mseati)\n",
    "# lr.bias\n",
    "# lr.coef.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = Mini_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.52785318e-11]),\n",
       " array([-0.04248061, -0.06166244, -0.1167385 ,  0.03905905, -5.62396234,\n",
       "         4.26276798,  2.53323108,  1.03571006,  2.04680922,  0.01260244]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb.fit(X_train, y_train,num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 11)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRCrossval(Normal_lr):\n",
    "    def __init__(self,max_iter = 3000,alpha = 0.001,batch_size = 50,cv = KFold(n_splits = 5)):\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self,X_train, y_train,num_epochs = 50,batch_size=50,type = 'batch'):\n",
    "\n",
    "        self.set_w = []\n",
    "        self.set_score = []\n",
    "        for train_idx,test_idx in self.cv.split(X_train):\n",
    "            X_cross_train = X_train[train_idx]\n",
    "            X_cross_test = X_train[test_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            y_cross_test = y_train[test_idx]\n",
    "            # print('hi')\n",
    "\n",
    "            self.theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "            for e in range(num_epochs):\n",
    "                permuted_index = np.random.permutation(X_cross_train.shape[0])\n",
    "                X_cross_train = X_cross_train[permuted_index]\n",
    "                y_cross_train = y_cross_train[permuted_index]\n",
    "\n",
    "                if type == 'normal':\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._run_grad(X_method_train,y_method_train,theta)\n",
    "                    pass\n",
    "\n",
    "                elif type == 'batch':\n",
    "                    for i in range(0,X_cross_train.shape[0],batch_size):\n",
    "                        X_method_train = X_cross_train[i:i+batch_size]\n",
    "                        y_method_train = y_cross_train[i:i+batch_size]\n",
    "                        self._run_grad(X_method_train,y_method_train,theta)\n",
    "\n",
    "                elif type == 'sto':\n",
    "                    for i in range(0,X_cross_train.shape[0],1):\n",
    "                        X_method_train = X_cross_train[i,:].reshape(1,-1)\n",
    "                        y_method_train = y_cross_train[i]\n",
    "                        self._run_grad(X_method_train,y_method_train,theta)\n",
    "                    pass\n",
    "                else:\n",
    "                    raise NameError\n",
    "\n",
    "            y_hat = self.predict(X_cross_test,self.theta)\n",
    "            score = self.cal_score(y_cross_test,y_hat)\n",
    "            self.set_score.append(score)\n",
    "            self.set_w.append(self.theta)\n",
    "            \n",
    "        return self.set_w,self.set_score\n",
    "\n",
    "\n",
    "        # self.coef = self.theta[1:]\n",
    "        # self.bias = self.theta[:1]\n",
    "           \n",
    "        # return self.bias,self.coef    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "lrcv = LRCrossval()\n",
    "a,b = lrcv.fit(X_train, y_train,type = 'sto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0.95358858, -27.86799329, -16.43601867, -14.35949376,\n",
       "        -25.44459267,   0.78682408,   1.22800691,  22.536712  ,\n",
       "        -13.18600173,  -6.85320478, -11.39900025]),\n",
       " array([  1.31952174,   6.31642439,  -1.93257278,  -7.30418712,\n",
       "          8.76540858, -20.24776133,  -1.97928323, -11.92016621,\n",
       "         -3.99383517, -14.57029701, -16.36392373]),\n",
       " array([  8.15516609, -10.96120327,  -4.74663515,   1.95894779,\n",
       "         29.75510076,   9.13618347,  10.13534394,  22.37907223,\n",
       "          1.4385966 ,  10.36050856,   2.40385016]),\n",
       " array([ -5.16648503,   7.56124136,   8.36363831, -23.23510276,\n",
       "        -25.67000797,   5.58369652,   6.30910324,   5.06984102,\n",
       "          5.73674359,  14.70129155, -17.84794069]),\n",
       " array([ -5.26179139,  24.78160837,  14.50493853,  42.47288187,\n",
       "         12.75032751, -17.75479209,   1.35790106, -27.93253473,\n",
       "         14.14733693,   4.54893854,  43.25742426])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53415.50376369131,\n",
       " 33575.02570632756,\n",
       " 25980.35902717449,\n",
       " 37323.815327491,\n",
       " 43018.87419883312]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai50')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
