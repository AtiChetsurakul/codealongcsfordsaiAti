{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now\n",
    "1. Python - general problem solving\n",
    "2. Data Science - NumPy, Pandas, Sklearn, Matplotlib \n",
    "3. ML from Scratch - Intuition (so for those who want to further advance....)\n",
    "4. Signal Processing - Energy, Telecommunciations, Biosignals, Time Series\n",
    "5. Deep Learning - PyTorch\n",
    "   1. One of the most popular DL framework (against TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "Good News\n",
    "- Deep Learning can automatically feature engineer / feature selection\n",
    "- Deep Learning can benefit from huge amount of data, while Machine Learning cannot\n",
    "  - 100 samples vs 1000 samples, ML will get the same accuracy\n",
    "  - But DL will see increased accuracy\n",
    "- Deep Learning is basically stacking a lot of linear regression together\n",
    "  - DL can learn very complex patterns\n",
    "  - DL is perfect for (1) images, (2) text, (3) signal (very random)\n",
    "\n",
    "Bad News\n",
    "- Deep Learning sucks with small data (vs. Machine Learning) - 5000++ samples\n",
    "- For Tabular Data, Deep Learning will ALMOST ALWAYS LOSE TO gradient boosting (or its variants)\n",
    "  - Gradient Boosting is basically decision trees stacking after one another....\n",
    "  - For most competition, XGBoost and LightGBM are always the winners for tabular data\n",
    "  - If you work in a company, mostly they use tabular data, then you should look for gradient boosting types.... \n",
    "- Deep Learning has NO feature importance; so it's mostly blackbox....(Explanable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pip install torch or pip3 install torch or conda install torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.22.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "PyTorch don't use NumPy.  Instead, it has its own data structures, called `Tensor`, which support automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch tensors from NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a numpy array of 1 to 5\n",
    "arr = np.arange(1, 6)\n",
    "# arr\n",
    "\n",
    "#print the data type\n",
    "arr.dtype  #int64\n",
    "\n",
    "#print the type()\n",
    "type(arr)  #belongs to Python itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert numpy to tensor\n",
    "\n",
    "#1. from_numpy (copy)\n",
    "torch_arr_from = torch.from_numpy(arr)\n",
    "torch_arr_from.dtype  #torch.int64\n",
    "type(torch_arr_from)  #torch.Tensor\n",
    "torch_arr_from.type() #torch.LongTensor (int64); if torch.IntTensor (int32)\n",
    "                      #torch.FloatTensor (float32); if torch.DoubleTensor (float64)\n",
    "#from_numpy is a copy!!!  This is intended, for easy use between numpy and tensor...\n",
    "# arr[2] = 999\n",
    "# torch_arr_from\n",
    "\n",
    "#2. tensor (not a copy)\n",
    "torch_arr_tensor = torch.tensor(arr)  #everything is the same, except it's NOT a copy\n",
    "arr[2] = 9999999\n",
    "torch_arr_tensor\n",
    "\n",
    "#In our class, mostly we use torch.tensor; it won't fail us :-)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some API to create tensor\n",
    "\n",
    "- `torch.empty(size)` - any arbitrary numbers\n",
    "- `torch.ones(size)`\n",
    "- `torch.zeros(size)`\n",
    "- `torch.arange(start, stop(ex), step)`\n",
    "- `torch.linspace(start, stop, how many)`\n",
    "- `torch.logspace(start, stop, how many)`  - power of 10\n",
    "    - `torch.rand(size)` - [0, 1)\n",
    "    - `torch.randn(size)` - std = 1 with uniform distribution\n",
    "    - `torch.randint(low, high, size)` - [low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009],\n",
      "        [0.8009]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#import some deep learning layer\n",
    "#you have to help me create the right shape to insert to this layer\n",
    "\n",
    "import torch.nn as nn  #nn contains a lot of useful deep learning layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 1)  #basically you insert 5 features, output 1 number\n",
    "linear_layer.weight.shape\n",
    "\n",
    "#can you guys help me generate any pytorch tensor of size (?, ?)\n",
    "data = torch.ones((12,5))\n",
    "output = linear_layer(data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1794]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1794]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_linear_layer = nn.Linear(100,5)\n",
    "output_linear_layer = nn.Linear(5,1)\n",
    "# all_layer = nn.Linear((100,5),1)\n",
    "# hidden_linear_layer.weight.shape, output_linear_layer.weight.shape\n",
    "data = torch.ones((1,100))\n",
    "output = output_linear_layer(hidden_linear_layer(data))\n",
    "print(output)\n",
    "\n",
    "model = nn.Sequential(hidden_linear_layer,output_linear_layer)\n",
    "print(model(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5881]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1,6)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "x = x.type(torch.float16)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)\n",
    "y = x.view(2,5)\n",
    "y[0,0] = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([999,   1,   2,   3,   4,   5,   6,   7,   8,   9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x # x,y share memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.reshape(2,5)\n",
    "z.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_t = z.transpose(1,0) # order by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai50')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da867d72de60a3e86a2b69a9a7baea090d67382d01a73f765a7401ae7e7cc0f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
